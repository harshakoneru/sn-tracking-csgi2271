{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/ByteTrack/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from loguru import logger\n",
    "import utils\n",
    "from external.adaptors import detector\n",
    "from trackers import integrated_ocsort_embedding as tracker_module\n",
    "\n",
    "from trackers.integrated_ocsort_embedding import cmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"cache/affine_ocsort_soccernet.pkl\", \"rb\") as fp:\n",
    "    cache = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def get_mot_loader(dataset, test, data_dir=\"/scratch/hk3820/ByteTrack/datasets\", workers=4, size=(800, 1440)):\n",
    "    # Different dataset paths\n",
    "    print(dataset == \"soccernet\")\n",
    "    if dataset == \"soccernet\":\n",
    "        direc = \"SoccerNet\"\n",
    "        if test:\n",
    "            name = \"test\"\n",
    "            annotation = \"test.json\"\n",
    "        else:\n",
    "            name = \"train\"\n",
    "            annotation = \"train.json\"\n",
    "    elif dataset == \"mot17\":\n",
    "        direc = \"mot\"\n",
    "        if test:\n",
    "            name = \"test\"\n",
    "            annotation = \"test.json\"\n",
    "        else:\n",
    "            name = \"train\"\n",
    "            annotation = \"val_half.json\"\n",
    "    elif dataset == \"mot20\":\n",
    "        direc = \"MOT20\"\n",
    "        if test:\n",
    "            name = \"test\"\n",
    "            annotation = \"test.json\"\n",
    "        else:\n",
    "            name = \"train\"\n",
    "            annotation = \"val_half.json\"\n",
    "    elif dataset == \"dance\":\n",
    "        direc = \"dancetrack\"\n",
    "        if test:\n",
    "            name = \"test\"\n",
    "            annotation = \"test.json\"\n",
    "        else:\n",
    "            annotation = \"val.json\"\n",
    "            name = \"val\"\n",
    "    else:\n",
    "        raise RuntimeError(\"Specify path here.\")\n",
    "\n",
    "    # Same validation loader for all MOT style datasets\n",
    "    valdataset = MOTDataset(\n",
    "        data_dir=os.path.join(data_dir, direc),\n",
    "        json_file=annotation,\n",
    "        img_size=size,\n",
    "        name=name,\n",
    "        preproc=ValTransform(\n",
    "            rgb_means=(0.485, 0.456, 0.406),\n",
    "            std=(0.229, 0.224, 0.225),\n",
    "        )\n",
    "        ## preproc=ValTransform(rgb_means=(0.0, 0.0, 0.0), std=(1.0, 1, 1.0),)\n",
    "    )\n",
    "\n",
    "    sampler = torch.utils.data.SequentialSampler(valdataset)\n",
    "    dataloader_kwargs = {\n",
    "        \"num_workers\": workers,\n",
    "        \"pin_memory\": True,\n",
    "        \"sampler\": sampler,\n",
    "    }\n",
    "    dataloader_kwargs[\"batch_size\"] = 16\n",
    "    val_loader = torch.utils.data.DataLoader(valdataset, **dataloader_kwargs)\n",
    "\n",
    "    return val_loader\n",
    "\n",
    "\n",
    "class MOTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    COCO dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        json_file=\"train_half.json\",\n",
    "        name=\"train\",\n",
    "        img_size=(608, 1088),\n",
    "        preproc=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        COCO dataset initialization. Annotation data are read into memory by COCO API.\n",
    "        Args:\n",
    "            data_dir (str): dataset root directory\n",
    "            json_file (str): COCO json file name\n",
    "            name (str): COCO data name (e.g. 'train2017' or 'val2017')\n",
    "            img_size (int): target image size after pre-processing\n",
    "            preproc: data augmentation strategy\n",
    "        \"\"\"\n",
    "        self.input_dim = img_size\n",
    "        self.data_dir = data_dir\n",
    "        self.json_file = json_file\n",
    "\n",
    "        self.coco = COCO(os.path.join(self.data_dir, \"annotations\", self.json_file))\n",
    "        self.ids = self.coco.getImgIds()\n",
    "        self.class_ids = sorted(self.coco.getCatIds())\n",
    "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        self._classes = tuple([c[\"name\"] for c in cats])\n",
    "        self.annotations = self._load_coco_annotations()\n",
    "        self.name = name\n",
    "        self.img_size = img_size\n",
    "        self.preproc = preproc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def _load_coco_annotations(self):\n",
    "        return [self.load_anno_from_ids(_ids) for _ids in self.ids]\n",
    "\n",
    "    def load_anno_from_ids(self, id_):\n",
    "        im_ann = self.coco.loadImgs(id_)[0]\n",
    "        width = im_ann[\"width\"]\n",
    "        height = im_ann[\"height\"]\n",
    "        frame_id = im_ann[\"frame_id\"]\n",
    "        video_id = im_ann[\"video_id\"]\n",
    "        anno_ids = self.coco.getAnnIds(imgIds=[int(id_)], iscrowd=False)\n",
    "        annotations = self.coco.loadAnns(anno_ids)\n",
    "        objs = []\n",
    "        for obj in annotations:\n",
    "            x1 = obj[\"bbox\"][0]\n",
    "            y1 = obj[\"bbox\"][1]\n",
    "            x2 = x1 + obj[\"bbox\"][2]\n",
    "            y2 = y1 + obj[\"bbox\"][3]\n",
    "            if obj[\"area\"] > 0 and x2 >= x1 and y2 >= y1:\n",
    "                obj[\"clean_bbox\"] = [x1, y1, x2, y2]\n",
    "                objs.append(obj)\n",
    "\n",
    "        num_objs = len(objs)\n",
    "\n",
    "        res = np.zeros((num_objs, 6))\n",
    "\n",
    "        for ix, obj in enumerate(objs):\n",
    "            cls = self.class_ids.index(obj[\"category_id\"])\n",
    "            res[ix, 0:4] = obj[\"clean_bbox\"]\n",
    "            res[ix, 4] = cls\n",
    "            res[ix, 5] = obj[\"track_id\"]\n",
    "\n",
    "        file_name = im_ann[\"file_name\"] if \"file_name\" in im_ann else \"{:012}\".format(id_) + \".jpg\"\n",
    "        img_info = (height, width, frame_id, video_id, file_name)\n",
    "\n",
    "        del im_ann, annotations\n",
    "\n",
    "        return (res, img_info, file_name)\n",
    "\n",
    "    def load_anno(self, index):\n",
    "        return self.annotations[index][0]\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        id_ = self.ids[index]\n",
    "\n",
    "        res, img_info, file_name = self.annotations[index]\n",
    "        # load image and preprocess\n",
    "        img_file = os.path.join(self.data_dir, self.name, file_name)\n",
    "        img = cv2.imread(img_file)\n",
    "\n",
    "        assert img is not None\n",
    "\n",
    "        return img, res.copy(), img_info, np.array([id_])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        One image / label pair for the given index is picked up and pre-processed.\n",
    "\n",
    "        Args:\n",
    "            index (int): data index\n",
    "\n",
    "        Returns:\n",
    "            img (numpy.ndarray): pre-processed image\n",
    "            padded_labels (torch.Tensor): pre-processed label data.\n",
    "                The shape is :math:`[max_labels, 5]`.\n",
    "                each label consists of [class, xc, yc, w, h]:\n",
    "                    class (float): class index.\n",
    "                    xc, yc (float) : center of bbox whose values range from 0 to 1.\n",
    "                    w, h (float) : size of bbox whose values range from 0 to 1.\n",
    "            info_img :\n",
    "                img_info = (height, width, frame_id, video_id, file_name)\n",
    "            img_id (int): same as the input index. Used for evaluation.\n",
    "        \"\"\"\n",
    "        img, target, img_info, img_id = self.pull_item(index)\n",
    "        tensor, target = self.preproc(img, target, self.input_dim)\n",
    "        return (tensor, img), target, img_info, img_id\n",
    "\n",
    "\n",
    "class ValTransform:\n",
    "    \"\"\"\n",
    "    Defines the transformations that should be applied to test PIL image\n",
    "    for input into the network\n",
    "\n",
    "    dimension -> tensorize -> color adj\n",
    "\n",
    "    Arguments:\n",
    "        resize (int): input dimension to SSD\n",
    "        rgb_means ((int,int,int)): average RGB of the dataset\n",
    "            (104,117,123)\n",
    "        swap ((int,int,int)): final order of channels\n",
    "\n",
    "    Returns:\n",
    "        transform (transform) : callable transform to be applied to test/val\n",
    "        data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rgb_means=None, std=None, swap=(2, 0, 1)):\n",
    "        self.means = rgb_means\n",
    "        self.swap = swap\n",
    "        self.std = std\n",
    "\n",
    "    # assume input is cv2 img for now\n",
    "    def __call__(self, img, res, input_size):\n",
    "        img, _ = preproc(img, input_size, self.means, self.std, self.swap)\n",
    "        return img, np.zeros((1, 5))\n",
    "\n",
    "\n",
    "def preproc(image, input_size, mean, std, swap=(2, 0, 1)):\n",
    "    if len(image.shape) == 3:\n",
    "        padded_img = np.ones((input_size[0], input_size[1], 3)) * 114.0\n",
    "    else:\n",
    "        padded_img = np.ones(input_size) * 114.0\n",
    "    img = np.array(image)\n",
    "    r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "    ).astype(np.float32)\n",
    "    padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img\n",
    "\n",
    "    padded_img = padded_img[:, :, ::-1]\n",
    "    padded_img /= 255.0\n",
    "    if mean is not None:\n",
    "        padded_img -= mean\n",
    "    if std is not None:\n",
    "        padded_img /= std\n",
    "    padded_img = padded_img.transpose(swap)\n",
    "    padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)\n",
    "    return padded_img, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_path = \"/scratch/hk3820/csgi2271_finalproject/YOLOX_outputs/yolox_x_10Ep_bytetrack_mot_17_run2/best_ckpt.pth.tar\"\n",
    "size = (896, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = detector.Detector(\"yolox\", detector_path, \"soccernet\")\n",
    "loader = get_mot_loader(\"soccernet\", True, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img, np_img), label, info, idx in loader:\n",
    "    pred = det(img.cuda())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ByteTrack",
   "language": "python",
   "name": "bytetrack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
